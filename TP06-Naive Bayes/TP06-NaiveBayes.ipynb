{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP06 : Naïve Bayes\n",
    "\n",
    "Tout le monde connait le théorème de Bayes pour calculer la probabilité conditionnelle d'un évennement $A$ sachant un autre $B$: \n",
    "$$ P(A|B) = \\frac{P(A)P(B|A)}{P(B)}$$\n",
    "\n",
    "Pour appliquer ce théorème sur un problème d'appentissage automatique, l'idée est simple ; Etant donné une caractéristique $f$ et la sortie $y$ qui peut avoir la classe $c$ : \n",
    "- Remplacer $A$ par $y=c$\n",
    "- Remplacer $B$ par $f$ \n",
    "On aura l'équation : \n",
    "$$ P(y=c|f) = \\frac{P(y=c)P(f|y=c)}{P(f)}$$\n",
    "\n",
    "On appelle : \n",
    "- $P(y=c|f)$ postérieure \n",
    "- $P(y=c)$ antérieure\n",
    "- $P(f|y=c)$ vraisemblance\n",
    "- $P(f)$ évidence \n",
    "\n",
    "Ici, on estime la probablité d'une classe $c$ sachant une caractéristique $f$ en utilisant des données d'entrainement. Maintenant, on veut estimer la probabilité d'une classe $c$ sachant un vecteur de caractéristiques $\\overrightarrow{f} = \\{f_1, ..., f_L\\}$ : \n",
    "$$ P(y=c|\\overrightarrow{f}) = \\frac{P(y=c)P(\\overrightarrow{f}|y=c)}{P(f)}$$\n",
    "\n",
    "Etant donnée plusieurs classes $c_j$, la classe choisie $\\hat{c}$ est celle avec la probabilité maximale \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} P(y=c_k|\\overrightarrow{f})$$\n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\frac{P(y=c_k)P(\\overrightarrow{f}|y=c_k)}{P(f)}$$\n",
    "On supprime l'évidence pour cacher le crime : $P(f)$ ne dépend pas de $c_k$ et elle est postive, donc ça ne va pas affecter la fonction $\\max$.\n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} P(y=c_k)P(\\overrightarrow{f}|y=c_k)$$\n",
    "\n",
    "Pour calculer $P(\\overrightarrow{f}|y=c_k)$, on va utiliser une properiété naïve (d'où vient le nom Naive Bayes) : on suppose l'indépendence conditionnelle entre les caractéristiques $f_j$. \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} P(y=c_k) \\prod\\limits_{f_j \\in \\overrightarrow{f}} P(f_j|y=c_k)$$\n",
    "\n",
    "Pour éviter la disparition de la probabilité (multiplication et représentation de virgule flottante sur machine), on transforme vers l'espace logarithme.\n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\log P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j|y=c_k)$$\n",
    "\n",
    "\n",
    "## Avantages \n",
    "\n",
    "Les classifieurs naïfs bayésiens, malgré leur simplicité, ont des points forts:\n",
    "- Ils ont besoin d'une petite quantité de données d'entrainement.\n",
    "- Ils sont très rapides par rapport aux autres classifieurs.\n",
    "- Ils donnent de bons résultats dans le cas de filtrage du courrier indésirable et de classification de documents.\n",
    "\n",
    "## Limites\n",
    "Les classifieurs naïfs bayésiens certes sont populaires à cause de leur simplicité. Mais, une telle simplicité vient avec un coût [référence: Spiderman].\n",
    "- Les probabilités obtenues en utilisant ces classifieurs ne doivent pas être prises au sérieux.\n",
    "- S'il existe une grande corrélation entre les caractéristiques, ils vont donner une mauvaise performance.\n",
    "- Dans le cas des caractéristiques continues (prix, surface, etc.), les données doivent suivre la loi normale.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I- Implémentation\n",
    "\n",
    "Pour estimer la vraisemblance, il y a plusieurs modèles (lois):\n",
    "- Loi multinomiale : pour les caracétristiques nominales\n",
    "- Loi de Bernoulli : lorsqu'on est interressé par l'apparence d'une caractéristique ou non (binaire)\n",
    "- loi normale : pour les caractéristiques numériques\n",
    "\n",
    "Dans ce TP, on va implémenter Naive Bayes pour les caractéristiques nominales (loi multinomiale)\n",
    "\n",
    "### I-1- Les données pour les tests unitaires\n",
    "Ici, on va utiliser le dataset \"jouer\" contenant des caractéristiques nominales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>temps</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidite</th>\n",
       "      <th>vent</th>\n",
       "      <th>jouer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>chaude</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>chaude</td>\n",
       "      <td>haute</td>\n",
       "      <td>oui</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>nuageux</td>\n",
       "      <td>chaude</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>oui</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>nuageux</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>oui</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>non</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>fraiche</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>douce</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>ensoleile</td>\n",
       "      <td>douce</td>\n",
       "      <td>normale</td>\n",
       "      <td>oui</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>nuageux</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>oui</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>nuageux</td>\n",
       "      <td>chaude</td>\n",
       "      <td>normale</td>\n",
       "      <td>non</td>\n",
       "      <td>oui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>pluvieux</td>\n",
       "      <td>douce</td>\n",
       "      <td>haute</td>\n",
       "      <td>oui</td>\n",
       "      <td>non</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        temps temperature humidite vent jouer\n",
       "0   ensoleile      chaude    haute  non   non\n",
       "1   ensoleile      chaude    haute  oui   non\n",
       "2     nuageux      chaude    haute  non   oui\n",
       "3    pluvieux       douce    haute  non   oui\n",
       "4    pluvieux     fraiche  normale  non   oui\n",
       "5    pluvieux     fraiche  normale  oui   non\n",
       "6     nuageux     fraiche  normale  oui   oui\n",
       "7   ensoleile       douce    haute  non   non\n",
       "8   ensoleile     fraiche  normale  non   oui\n",
       "9    pluvieux       douce  normale  non   oui\n",
       "10  ensoleile       douce  normale  oui   oui\n",
       "11    nuageux       douce    haute  oui   oui\n",
       "12    nuageux      chaude  normale  non   oui\n",
       "13   pluvieux       douce    haute  oui   non"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "\n",
    "jouer = pd.read_csv(\"datasets/jouer.csv\")\n",
    "\n",
    "X_jouer = jouer.iloc[:, :-1].values # Premières colonnes \n",
    "Y_jouer = jouer.iloc[:,-1].values # Dernière colonne \n",
    "\n",
    "# Afficher le dataset \"jouer\"\n",
    "jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-2- Estimation de la probabilité antérieure\n",
    "Etant donné le vecteur de sortie $Y$, on doit calculer la probabilité de chaque classe (différentes valeurs de $Y$)\n",
    "\n",
    "$$p(c_k) = \\frac{|\\{y / y \\in Y \\text{ et } y = c_k\\}|}{|Y|}$$\n",
    "\n",
    "La fonction doit retourner un dictionnaire où la clé est le nom de la classe et la valeur est sa probabilité. Voici, un exemple d'un dictionnaire dans Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(Iris-setosa)= 0.5\n",
      "P(Iris-versicolor)= 0.33\n",
      "P(Iris-virginica)= 0.67\n"
     ]
    }
   ],
   "source": [
    "# Exemple de dictionnaire dans Python \n",
    "d = {}\n",
    "d[\"Iris-setosa\"] = 0.5\n",
    "d[\"Iris-versicolor\"] = 0.33\n",
    "d[\"Iris-virginica\"] = 0.67\n",
    "\n",
    "for c in d: \n",
    "    print(\"P(\" + c + \")= \" + str(d[c]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'non': 0.35714285714285715, 'oui': 0.6428571428571429}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Réaliser la fonction \n",
    "def P_c(Y): \n",
    "    keys,counts = np.unique(Y,return_counts=True)\n",
    "    resultat = {}\n",
    "    for i in range(len(keys)):\n",
    "        resultat[keys[i]] = counts[i] / len(Y)\n",
    "    return resultat\n",
    "\n",
    "# Résultat: {'non': 0.35714285714285715, 'oui': 0.6428571428571429}\n",
    "P_c(Y_jouer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-3- Entrainement  (loi multinomiale)\n",
    "\n",
    "Notre modèle (notons le par $\\theta_{f_j,C}$) doit garder le nombre des différentes valeurs dans une caractéristique $A$ et le nombre de ces valeurs dans chaque classe.\n",
    "\n",
    "Donc, étant donné un vecteur d'une caractéristique $A$ et un autre des $Y$ respectives, la fonction d'entrainement doit retourner un dictionnaire (notre théta) : \n",
    "- la clé est une valeur $a_v$ de $A$ \n",
    "- la valeur est un autre dictionnaire : \n",
    "   - il doit contenir une clé \"_total_\" dont la valeur est le nombre d'occurence de $a_v$ dans $A$ \n",
    "   - la clé est la classe $c_k$ de $Y$\n",
    "   - la valeur est le nombre d'occurence de $a_v$ respectives à $c_k$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ensoleile': {'_total_': 5, 'non': 3, 'oui': 2},\n",
       " 'nuageux': {'_total_': 4, 'non': 0, 'oui': 4},\n",
       " 'pluvieux': {'_total_': 5, 'non': 2, 'oui': 3}}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO Réaliser cette fonction \n",
    "# Elle génère théta pour une seule caractéristique\n",
    "def entrainer_multi_1(A, Y): \n",
    "    keys,counts = np.unique(A,return_counts=True)\n",
    "    y_keys,y_counts = np.unique(Y,return_counts=True)\n",
    "    resultat = {}\n",
    "    sub_y = sub_y_keys = sub_y_counts = []\n",
    "    for i in range(len(keys)):\n",
    "        sub_y = [ val for (index,val) in enumerate(Y) if A[index] == keys[i]]\n",
    "        sub_y_keys,sub_y_counts = np.unique(sub_y,return_counts=True)\n",
    "        sub_result = {\n",
    "            '_total_' : counts[i]\n",
    "        }\n",
    "        for k in range(len(y_keys)):\n",
    "            sub_result[y_keys[k]] = 0\n",
    "            \n",
    "        for j in range(len(sub_y_keys)):\n",
    "            sub_result[sub_y_keys[j]] = sub_y_counts[j] \n",
    "        \n",
    "        resultat[keys[i]] = sub_result\n",
    "    return resultat\n",
    "\n",
    "# Résultat \n",
    "# {             'ensoleile': {'_total_': 5, 'non': 3, 'oui': 2},\n",
    "#               'nuageux'  : {'_total_': 4, 'non': 0, 'oui': 4},\n",
    "#               'pluvieux' : {'_total_': 5, 'non': 2, 'oui': 3}\n",
    "# }\n",
    "Theta_jouer_temps = entrainer_multi_1(X_jouer[:, 0], Y_jouer)\n",
    "\n",
    "Theta_jouer_temps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'ensoleile': {'_total_': 5, 'non': 3, 'oui': 2},\n",
       "  'nuageux': {'_total_': 4, 'non': 0, 'oui': 4},\n",
       "  'pluvieux': {'_total_': 5, 'non': 2, 'oui': 3}},\n",
       " {'chaude': {'_total_': 4, 'non': 2, 'oui': 2},\n",
       "  'douce': {'_total_': 6, 'non': 2, 'oui': 4},\n",
       "  'fraiche': {'_total_': 4, 'non': 1, 'oui': 3}},\n",
       " {'haute': {'_total_': 7, 'non': 4, 'oui': 3},\n",
       "  'normale': {'_total_': 7, 'non': 1, 'oui': 6}},\n",
       " {'non': {'_total_': 8, 'non': 2, 'oui': 6},\n",
       "  'oui': {'_total_': 6, 'non': 3, 'oui': 3}},\n",
       " {'non': 0.35714285714285715, 'oui': 0.6428571428571429}]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# La fonction qui entraine Théta sur plusieurs caractéristiques\n",
    "# Rien à programmer ici\n",
    "# Notre théta est une liste des dictionnaires;\n",
    "# chaque dictionnaire contient le théta de la caractéristique respective à la colonne de X\n",
    "# On ajoute les probabilités antérieures des classes à la fin de résultat\n",
    "def entrainer_multi(X, Y): \n",
    "    resultat = []\n",
    "    for i in range(X.shape[1]): \n",
    "        resultat.append(entrainer_multi_1(X[:, i], Y))\n",
    "    resultat.append(P_c(Y))\n",
    "    return resultat\n",
    "\n",
    "Theta_jouer = entrainer_multi(X_jouer, Y_jouer)\n",
    "\n",
    "Theta_jouer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-4- Estimation de la probabilité de vraissemblance (loi multinomiale)\n",
    "L'équation pour estimer la vraisemblance \n",
    "$$ P(f_j=v|y=c_k) = \\frac{|\\{ y \\in Y / y = c_k \\text{ et } f_j = v\\}|}{|\\{y = c_k\\}|}$$\n",
    "\n",
    "Si, dans le dataset de test, on veut calculer la probabilité d'une valeur $v$ qui n'existe pas dans le dataset d'entrainnement ou qui n'existe pas pour une classe donnée, on aura une probabilité nulle. Ici, on doit appliquer une fonction de lissage qui donne une petite probabilité aux données non vues dans l'entrainnement. Le lissage qu'on va utiliser est celui de Lidstone. Lorsque $\\alpha = 1$ on l'appelle lissage de Laplace.\n",
    "$$ P(f_j=v|y=c_k) = \\frac{|\\{ y \\in Y / y = c_k \\text{ et } f_j = v\\}| + \\alpha}{|\\{y = c_k\\}| + \\alpha * |V|}$$\n",
    "Où: \n",
    "- $\\alpha$ est une valeur donnée \n",
    "- $V$ est l'ensemble des différentes valeurs de $f_j$ (le vocabulaire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333, 0.08333333333333333)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO compléter cette fonction\n",
    "def P_vraiss_multi(Theta_j, v, c, alpha=1.): \n",
    "    len_V = len(Theta_j) # La taille du vocabulaire\n",
    "    nbr_c= nbr_v = 0\n",
    "    for i in Theta_j.keys() :\n",
    "        nbr_c += Theta_j[i][c]\n",
    "        if i == v:\n",
    "            nbr_v += Theta_j[i][c]\n",
    "    return (nbr_v + alpha) / ( nbr_c + alpha * len_V )\n",
    "\n",
    "# La probabilité de jouer si temps = pluvieux \n",
    "# P(temps = pluvieux | jouer=oui) = (nbr(temps=pluvieux et jouer=oui)+alpha)/(nbr(jour=oui) + alpha * nbr_diff(temps)))\n",
    "# P(temps = pluvieux | jouer=oui) = (3 + 1)/(9 + 3) ==> 3 est le nombre de différentes valeurs de temps (entrainnement)\n",
    "# P(temps = pluvieux | jouer=oui) = 4/12 ==> 0.33333333333333333333333333333333333~\n",
    "\n",
    "# La probabilité de jouer si temps = neigeux \n",
    "# P(temps = neigeux | jouer=oui) = (nbr(temps=neigeux et jouer=oui)+alpha)/(nbr(jouer=oui) + alpha * nbr_diff(temps)))\n",
    "# P(temps = neigeux | jouer=oui) = (0 + 1)/(9 + 3) ==> 3 est le nombre de différentes valeurs de temps (entrainnement)\n",
    "# P(temps = neigeux | jouer=oui) = 1/13 ==> 0.0833333333333333333333333333333333333~\n",
    "\n",
    "\n",
    "P_vraiss_multi(Theta_jouer_temps, \"pluvieux\", \"oui\"), P_vraiss_multi(Theta_jouer_temps, \"neigeux\", \"oui\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-5- Prédiction de la classe (loi multinomiale)\n",
    "Revenons maintenant à notre équation de prédiction \n",
    "$$\\hat{c} = \\arg\\max\\limits_{c_k} \\log P(y=c_k) + \\sum\\limits_{f_j \\in \\overrightarrow{f}} \\log P(f_j|y=c_k)$$\n",
    "\n",
    "Ici, vous devez prédire un seule échantillon $x$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(('oui', -4.102643365036796), ('oui', -3.6608106127577567))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO compléter ce code\n",
    "# Pour récupérer le théta de la caractéristique n°0 : Theta[0]\n",
    "# anter est un booléen, si il est False, on ne compte pas la probabilité antérieure P(y = c_k)\n",
    "def predire(x, Theta, alpha=1., anter=True): \n",
    "    c_opt = \"\" # la classe optimale\n",
    "    p_c = Theta[-1] #les classes et leurs probabilités antérieures\n",
    "    if not anter: # si on ne veut pas ajouter les probabiliés antérieures\n",
    "        p_c = dict.fromkeys(p_c, 1.) # on définit le tous en 1; log(1) = 0\n",
    "    max_log_p = np.NINF # - infinity \n",
    "   \n",
    "    for index,key in enumerate(p_c.keys()):\n",
    "        s = np.log(p_c[key])\n",
    "        for i,j in enumerate(x):\n",
    "            s += np.log(P_vraiss_multi(Theta[i],j,key,alpha))\n",
    "        if s > max_log_p :\n",
    "            max_log_p ,c_opt = s ,key \n",
    "        \n",
    "    return c_opt, max_log_p\n",
    "\n",
    "# Résultat: (('oui', -4.102643365036796), ('oui', -3.6608106127577567))\n",
    "predire([\"pluvieux\", \"fraiche\", \"normale\", \"oui\"], Theta_jouer), predire([\"pluvieux\", \"fraiche\", \"normale\", \"oui\"], Theta_jouer, anter=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I-7- Regrouper en une classe (loi multinomiale)\n",
    "\n",
    "**Rien à programmer ici, il y a une petite analyse**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBMultinom(object): \n",
    "    \n",
    "    def __init__(self, alpha=1.): \n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def entrainer(self, X, Y):\n",
    "        self.Theta = entrainer_multi(X, Y)\n",
    "    \n",
    "    def predire(self, X, anter=True, prob=False): \n",
    "        Y_pred = []\n",
    "        for i in range(len(X)): \n",
    "            c, p = predire(X[i,:], self.Theta, alpha=self.alpha, anter=anter)\n",
    "            if prob:\n",
    "                Y_pred.append(p)\n",
    "            else:\n",
    "                Y_pred.append(c)\n",
    "        return Y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va entrainer un modèle en utilisant notre imlémentation avec et sans probabilité antérieure. \n",
    "Normalement, on doit tester sur des données non vues (des données qu'on n'a pas utilisé pour l'entrainement). Mais, ici, on va tester sur les mêmes données d'entrainement afin de savoir si le modèle a bien représenté ce dataset ou non (calculer l'erreur) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notre modèle avec probabilité antérieure (a priori)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         non       0.80      1.00      0.89         4\n",
      "         oui       1.00      0.90      0.95        10\n",
      "\n",
      "    accuracy                           0.93        14\n",
      "   macro avg       0.90      0.95      0.92        14\n",
      "weighted avg       0.94      0.93      0.93        14\n",
      "\n",
      "Notre modèle sans probabilité antérieure (a priori)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         non       0.80      0.67      0.73         6\n",
      "         oui       0.78      0.88      0.82         8\n",
      "\n",
      "    accuracy                           0.79        14\n",
      "   macro avg       0.79      0.77      0.78        14\n",
      "weighted avg       0.79      0.79      0.78        14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "notre_modele = NBMultinom()\n",
    "notre_modele.entrainer(X_jouer, Y_jouer)\n",
    "Y_notre_ant = notre_modele.predire(X_jouer)\n",
    "Y_notre_sans_ant = notre_modele.predire(X_jouer, anter=False)\n",
    "\n",
    "# Ici, ce n'ai pas la peine d'exécuter plusieurs fois\n",
    "# puisque le résultat sera le même \n",
    "\n",
    "# Le rapport de classification\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(\"Notre modèle avec probabilité antérieure (a priori)\")\n",
    "print(classification_report(Y_notre_ant, Y_jouer))\n",
    "\n",
    "print(\"Notre modèle sans probabilité antérieure (a priori)\")\n",
    "print(classification_report(Y_notre_sans_ant, Y_jouer))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser les résultats** \n",
    "<br>\n",
    "On remarque que:\n",
    "- Les deux modèles ont bien représenté le dataset\n",
    "- Le modèle avec probabilité antérieure présente de meilleurs résultat que l'autre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II- Détection de spam \n",
    "\n",
    "Ici, on va essayer d'appliquer l'apprentissage automatique sur la détection de spam. \n",
    "Chaque message dans le dataset est représenté en utilisant un modèle \"Sac à mots\" (BoW : Bag of Words).\n",
    "Dans l'entrainement, on récupère les différents mots qui s'apparaissent dans les messages. \n",
    "Chaque mot va être considéré comme une caractéristique. \n",
    "Donc, pour chaque message, la valeur de la caractéristique est la fréquence de son mot dans le message. \n",
    "Par exemple, si le mot \"good\" apparait 3 fois dans le message, donc la caractéristique \"good\" aura la valeur 3 dans ce message.\n",
    "\n",
    "Notre implémentation n'est pas adéquate pour la nature de ce problème. \n",
    "Dans Scikit-learn, le [sklearn.naive_bayes.CategoricalNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.CategoricalNB.html) est similaire à notre implémentation. \n",
    "L'algorithme adéquat pour ce type de problème est [sklearn.naive_bayes.MultinomialNB](https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html).\n",
    "\n",
    "Le dataset utilisé est [SMS Spam Collection Dataset](https://www.kaggle.com/uciml/sms-spam-collection-dataset).\n",
    "Les algorithmes comparés :\n",
    "- Naive Bayes\n",
    "- Arbre de décision\n",
    "- Regression logistique \n",
    "\n",
    "### II-1- Préparation de données\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>texte</th>\n",
       "      <th>classe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "      <td>spam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "      <td>ham</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               texte classe\n",
       "0  Go until jurong point, crazy.. Available only ...    ham\n",
       "1                      Ok lar... Joking wif u oni...    ham\n",
       "2  Free entry in 2 a wkly comp to win FA Cup fina...   spam\n",
       "3  U dun say so early hor... U c already then say...    ham\n",
       "4  Nah I don't think he goes to usf, he lives aro...    ham"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = pd.read_csv(\"datasets/spam.csv\", encoding=\"latin-1\")\n",
    "messages = messages.rename(columns={\"v1\": \"classe\", \"v2\": \"texte\"})\n",
    "messages = messages.filter([\"texte\", \"classe\"])\n",
    "\n",
    "messages.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-2- Entrainement et test des modèles sur plusieurs exécutions \n",
    "\n",
    "Afin de satisfaire un étudiant qui réclame toujours sur le manque des données, nous avons décidé de comparer les algorithmes sur plusieurs excécutions (runs). \n",
    "\n",
    "**Rien à analyser ici**\n",
    "\n",
    "**P.S.** timeit.default_timer() est dépendante du système d'exploitation. Aussi, elle peut être affectée par d'autre processus en parallèle. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'naive_bayes': [0.007559700001365854,\n",
       "  0.007837999999537715,\n",
       "  0.007951999999932013,\n",
       "  0.008776999999099644,\n",
       "  0.013415100000202074,\n",
       "  0.015027999999801978,\n",
       "  0.008826700001009158],\n",
       " 'arbre_decision': [0.09238189999996393,\n",
       "  0.09128590000000258,\n",
       "  0.105362199999945,\n",
       "  0.11421980000159238,\n",
       "  0.10148550000121759,\n",
       "  0.10758549999991374,\n",
       "  0.11417330000040238],\n",
       " 'reg_log': [0.05187370000021474,\n",
       "  0.0499601999999868,\n",
       "  0.048833900000317954,\n",
       "  0.08134880000034173,\n",
       "  0.05943159999878844,\n",
       "  0.04652689999966242,\n",
       "  0.07069619999856513]}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import timeit\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "NBR_RUN = 7\n",
    "\n",
    "temps_train = {\n",
    "    \"naive_bayes\" : [],\n",
    "    \"arbre_decision\": [],\n",
    "    \"reg_log\": []\n",
    "}\n",
    "\n",
    "temps_test = {\n",
    "    \"naive_bayes\" : [],\n",
    "    \"arbre_decision\": [],\n",
    "    \"reg_log\": []\n",
    "}\n",
    "\n",
    "perf = {\n",
    "    \"naive_bayes_P\" : [],\n",
    "    \"arbre_decision_P\": [],\n",
    "    \"reg_log_P\": [], \n",
    "    \"naive_bayes_R\" : [],\n",
    "    \"arbre_decision_R\": [],\n",
    "    \"reg_log_R\": []\n",
    "}\n",
    "\n",
    "\n",
    "for run in range(NBR_RUN): \n",
    "    # prétaitement des données\n",
    "    msg_train, msg_test, Y_train, Y_test = train_test_split(messages[\"texte\"],messages[\"classe\"],test_size=0.2)\n",
    "    count_vectorizer = CountVectorizer()\n",
    "    X_train = count_vectorizer.fit_transform(msg_train)\n",
    "    X_test = count_vectorizer.transform(msg_test)\n",
    "    \n",
    "    # ==================================\n",
    "    # ENTRAINEMENT \n",
    "    # ==================================\n",
    "    \n",
    "    #entrainement Naive Bayes\n",
    "    naive_bayes = MultinomialNB()\n",
    "    temps_debut = timeit.default_timer()\n",
    "    naive_bayes.fit(X_train, Y_train)\n",
    "    temps_train[\"naive_bayes\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    #entrainement CART\n",
    "    arbre_decision = DecisionTreeClassifier()\n",
    "    temps_debut = timeit.default_timer()\n",
    "    arbre_decision.fit(X_train, Y_train)\n",
    "    temps_train[\"arbre_decision\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    #entrainement Régression logitique\n",
    "    reg_log = LogisticRegression(solver=\"lbfgs\") #solver=sag est plus lent; donc j'ai choisi le plus rapide\n",
    "    temps_debut = timeit.default_timer()\n",
    "    reg_log.fit(X_train, Y_train)\n",
    "    temps_train[\"reg_log\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    # ==================================\n",
    "    # TEST \n",
    "    # ==================================\n",
    "    \n",
    "    #test Naive Bayes\n",
    "    temps_debut = timeit.default_timer()\n",
    "    Y_naive_bayes = naive_bayes.predict(X_test)\n",
    "    temps_test[\"naive_bayes\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    \n",
    "    #test CART\n",
    "    temps_debut = timeit.default_timer()\n",
    "    Y_arbre_decision = arbre_decision.predict(X_test)\n",
    "    temps_test[\"arbre_decision\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    #test Régression logitique\n",
    "    temps_debut = timeit.default_timer()\n",
    "    Y_reg_log = reg_log.predict(X_test)\n",
    "    temps_test[\"reg_log\"].append(timeit.default_timer() - temps_debut)\n",
    "    \n",
    "    # ==================================\n",
    "    # PERFORMANCE \n",
    "    # ==================================\n",
    "    # Ici, on va considérer une classification binaire avec une seule classe \"spam\" \n",
    "    # On ne juge pas le classifieur sur sa capacité de détecter les non spams\n",
    "    \n",
    "    perf[\"naive_bayes_P\"].append(precision_score(Y_test, Y_naive_bayes, pos_label=\"spam\"))\n",
    "    perf[\"arbre_decision_P\"].append(precision_score(Y_test, Y_arbre_decision, pos_label=\"spam\"))\n",
    "    perf[\"reg_log_P\"].append(precision_score(Y_test, Y_reg_log, pos_label=\"spam\"))\n",
    "    \n",
    "    perf[\"naive_bayes_R\"].append(recall_score(Y_test, Y_naive_bayes, pos_label=\"spam\"))\n",
    "    perf[\"arbre_decision_R\"].append(recall_score(Y_test, Y_arbre_decision, pos_label=\"spam\"))\n",
    "    perf[\"reg_log_R\"].append(recall_score(Y_test, Y_reg_log, pos_label=\"spam\"))\n",
    "    \n",
    "    \n",
    "\n",
    "temps_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-3- Analyse du temps d'apprentissage \n",
    "\n",
    "Combien de temps chaque algorithme prend pour entrainer le même dataset d'entrainement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>naive_bayes</th>\n",
       "      <th>arbre_decision</th>\n",
       "      <th>reg_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.007560</td>\n",
       "      <td>0.092382</td>\n",
       "      <td>0.051874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.007838</td>\n",
       "      <td>0.091286</td>\n",
       "      <td>0.049960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.007952</td>\n",
       "      <td>0.105362</td>\n",
       "      <td>0.048834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.008777</td>\n",
       "      <td>0.114220</td>\n",
       "      <td>0.081349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.013415</td>\n",
       "      <td>0.101486</td>\n",
       "      <td>0.059432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.015028</td>\n",
       "      <td>0.107585</td>\n",
       "      <td>0.046527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.008827</td>\n",
       "      <td>0.114173</td>\n",
       "      <td>0.070696</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   naive_bayes  arbre_decision   reg_log\n",
       "0     0.007560        0.092382  0.051874\n",
       "1     0.007838        0.091286  0.049960\n",
       "2     0.007952        0.105362  0.048834\n",
       "3     0.008777        0.114220  0.081349\n",
       "4     0.013415        0.101486  0.059432\n",
       "5     0.015028        0.107585  0.046527\n",
       "6     0.008827        0.114173  0.070696"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(temps_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser**\n",
    "En comparant les temps d'apprentissage de chaque algorithme sur ce dataset on constate que :\n",
    "<br>\n",
    "- Le classifieur Naive bayse présente les meilleurs temps d'entrainement, ceci confirme le fait que cet algorithme est adapté pour le filtrage du courrier indésirable et de classification de documents.\n",
    "<br>\n",
    "- La régression logistique aussi présente de bons temps d'entrainement dans ce cas là, contrairement aux arbres de décision qui prennent un temps relativement plus élévé, et celà est du au nombre important de caractéristique (features), vu que chaque mot représente une."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-4- Analyse du temps de test \n",
    "\n",
    "Combien de temps chaque algorithme prend pour prédir les classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>naive_bayes</th>\n",
       "      <th>arbre_decision</th>\n",
       "      <th>reg_log</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.000196</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.000227</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000090</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.000402</td>\n",
       "      <td>0.000648</td>\n",
       "      <td>0.000192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.000376</td>\n",
       "      <td>0.000817</td>\n",
       "      <td>0.000183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.000335</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.000195</td>\n",
       "      <td>0.000357</td>\n",
       "      <td>0.000088</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.000191</td>\n",
       "      <td>0.000363</td>\n",
       "      <td>0.000092</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   naive_bayes  arbre_decision   reg_log\n",
       "0     0.000196        0.000357  0.000089\n",
       "1     0.000227        0.000357  0.000090\n",
       "2     0.000402        0.000648  0.000192\n",
       "3     0.000376        0.000817  0.000183\n",
       "4     0.000335        0.000505  0.000139\n",
       "5     0.000195        0.000357  0.000088\n",
       "6     0.000191        0.000363  0.000092"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(temps_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser**\n",
    "<br>\n",
    "Maintenant en comparant les temps de prédiction de chaque algorithme:\n",
    "- la régression logistique présente les meilleurs résultat, vu que la prédiction pour cet algorithme, ne s'agit que de calculer une valeur en utilisant le vecteur des theta et la comparer avec un seuil pour pouvoir la classifier ( Ici il s'agit d'une regression logistique binaire spam / non spam )\n",
    "- La classifieur naive bayes vient en deuxième position, car pour prédire il faut calculer la vraissemblance de chacune des caractéristiques pour chaque valeur de y, et l'antérieure des valeurs des y.\n",
    "- Les arbre de décision, dans ce cas l'algorithme CART vient en dernier pour ce dataset, car pour prédire il faut parcourir l'arbre jusqu'à arriver à une feuille, et vu le nombre des caractéristique (Chaque mot représente une), l'arbre est trop profond."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II-5- Analyse de la performance \n",
    "\n",
    "Ici, on compare les modèles en se basant sur leurs capacités à détecter le spam. \n",
    "On va utiliser la précision et le rappel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>arbre_decision_P</th>\n",
       "      <th>naive_bayes_P</th>\n",
       "      <th>reg_log_P</th>\n",
       "      <th>arbre_decision_R</th>\n",
       "      <th>naive_bayes_R</th>\n",
       "      <th>reg_log_R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.894737</td>\n",
       "      <td>0.978102</td>\n",
       "      <td>0.976378</td>\n",
       "      <td>0.843972</td>\n",
       "      <td>0.950355</td>\n",
       "      <td>0.879433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.885906</td>\n",
       "      <td>0.980000</td>\n",
       "      <td>0.986014</td>\n",
       "      <td>0.851613</td>\n",
       "      <td>0.948387</td>\n",
       "      <td>0.909677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.937931</td>\n",
       "      <td>0.979452</td>\n",
       "      <td>0.992857</td>\n",
       "      <td>0.860759</td>\n",
       "      <td>0.905063</td>\n",
       "      <td>0.879747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.872483</td>\n",
       "      <td>0.985714</td>\n",
       "      <td>0.984733</td>\n",
       "      <td>0.860927</td>\n",
       "      <td>0.913907</td>\n",
       "      <td>0.854305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.915385</td>\n",
       "      <td>0.968504</td>\n",
       "      <td>0.967213</td>\n",
       "      <td>0.875000</td>\n",
       "      <td>0.904412</td>\n",
       "      <td>0.867647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.926174</td>\n",
       "      <td>0.986755</td>\n",
       "      <td>0.979021</td>\n",
       "      <td>0.890323</td>\n",
       "      <td>0.961290</td>\n",
       "      <td>0.903226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.869565</td>\n",
       "      <td>0.984252</td>\n",
       "      <td>0.975806</td>\n",
       "      <td>0.882353</td>\n",
       "      <td>0.919118</td>\n",
       "      <td>0.889706</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   arbre_decision_P  naive_bayes_P  reg_log_P  arbre_decision_R  \\\n",
       "0          0.894737       0.978102   0.976378          0.843972   \n",
       "1          0.885906       0.980000   0.986014          0.851613   \n",
       "2          0.937931       0.979452   0.992857          0.860759   \n",
       "3          0.872483       0.985714   0.984733          0.860927   \n",
       "4          0.915385       0.968504   0.967213          0.875000   \n",
       "5          0.926174       0.986755   0.979021          0.890323   \n",
       "6          0.869565       0.984252   0.975806          0.882353   \n",
       "\n",
       "   naive_bayes_R  reg_log_R  \n",
       "0       0.950355   0.879433  \n",
       "1       0.948387   0.909677  \n",
       "2       0.905063   0.879747  \n",
       "3       0.913907   0.854305  \n",
       "4       0.904412   0.867647  \n",
       "5       0.961290   0.903226  \n",
       "6       0.919118   0.889706  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(perf, columns = [\"arbre_decision_P\", \"naive_bayes_P\", \"reg_log_P\", \"arbre_decision_R\", \"naive_bayes_R\", \"reg_log_R\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "arbre_decision_P    0.900312\n",
       "naive_bayes_P       0.980397\n",
       "reg_log_P           0.980289\n",
       "arbre_decision_R    0.866421\n",
       "naive_bayes_R       0.928933\n",
       "reg_log_R           0.883391\n",
       "dtype: float64"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Analyser**\n",
    "\n",
    "- Pour la précision, Le naive bayes et la regression logistique présente des résultats presque similaires et qui sont mieux que ceux présenté par les arbres de décision\n",
    "- Pour le recall, c'est le naive bayes qui a les meilleurs résultats, et puis vient la regression logistique et en dernier les arbres de décision.\n",
    "- Comme conclusion, on peut dire que les arbres de décision ne sont pas de bons classifieur lorsque il s'agit du text. Sinon entre la regression logistique et le naive bayes, si on préfère un meilleur recall, on doit choisir le naive bayes, et si on prefère une prédiction plus rapide on doit choisir la regression logistique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
